{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1753b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from src.rl_agent import RLAgent\n",
    "from src.rl_experiments import RLExperiments\n",
    "from src.state_representation import StateRepresentation\n",
    "\n",
    "from src.environments.env_pendulum import EnvironmentPendulum\n",
    "from src.environments.env_redpillbluepill import EnvironmentRedPillBluePill\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# pytorch_device = 'cuda'\n",
    "# pytorch_device = 'mps'\n",
    "pytorch_device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278ee01e",
   "metadata": {},
   "source": [
    "## Inverted Pendulum (CVaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3cde50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = EnvironmentPendulum(render_mode=None) \n",
    "\n",
    "# define agent with function approximation\n",
    "num_tiles = 8\n",
    "num_tilings = 32\n",
    "iht_size = 4096\n",
    "state_limits = []\n",
    "for i in range(env.gym_env.observation_space.shape[0]):\n",
    "    state_limits.append([\n",
    "        env.gym_env.observation_space.low[i],\n",
    "        env.gym_env.observation_space.high[i],\n",
    "    ])\n",
    "\n",
    "state_representation = StateRepresentation(method='tilecoding',\n",
    "                                           settings={\n",
    "                                               'num_tiles': num_tiles,\n",
    "                                               'num_tilings': num_tilings,\n",
    "                                               'iht_size': iht_size,\n",
    "                                               'state_limits': state_limits,\n",
    "                                           })\n",
    "\n",
    "states = np.zeros((iht_size, 1))\n",
    "\n",
    "actions = list(env.action_dict.keys())\n",
    "\n",
    "policy = {\n",
    "    'initialize': {\n",
    "        'file_path': './src/pytorch_networks/linear_discrete_actions.py',\n",
    "    },\n",
    "}\n",
    "\n",
    "value_network = {\n",
    "    'initialize': {\n",
    "        'file_path': './src/pytorch_networks/linear_discrete_actions.py',\n",
    "    }, \n",
    "}\n",
    "\n",
    "agent = RLAgent(agent_type='ac', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                avg_reward_method='differential',\n",
    "                initial_avg_reward=0.0,\n",
    "                action_type='discrete',\n",
    "                action_selection_rule='softmax',\n",
    "                policy_type='nn_pytorch',\n",
    "                policy_update_type='stochastic_gradient_descent',\n",
    "                policy_loss='ac_policy_loss',\n",
    "                value_type='nn_pytorch', \n",
    "                value_network=value_network,\n",
    "                value_update_type='stochastic_gradient_descent',\n",
    "                value_loss='mse_loss',\n",
    "                pytorch_device=pytorch_device,\n",
    "                use_cvar=True, \n",
    "                var_quantile=0.1, \n",
    "                initial_var_reward=0.0,\n",
    "               )\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "\n",
    "step_sizes = {\n",
    "    'value': 0.002,\n",
    "    'policy': 1,\n",
    "    'avg_reward': 0.01,\n",
    "    'var': 0.001,\n",
    "}\n",
    "\n",
    "df_pendulum_cvar = rl_experiments.run_experiment_continuing(experiment='pendulum_cvar',\n",
    "                                                            agent=agent, \n",
    "                                                            env=env,\n",
    "                                                            state_representation=state_representation,\n",
    "                                                            num_runs=10,\n",
    "                                                            max_steps=25000,\n",
    "                                                            discount=1.0,\n",
    "                                                            step_size=step_sizes,\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0515d3",
   "metadata": {},
   "source": [
    "## Inverted Pendulum (Differential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cbfcff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = EnvironmentPendulum(render_mode=None) \n",
    "\n",
    "# define agent with function approximation\n",
    "num_tiles = 8\n",
    "num_tilings = 32\n",
    "iht_size = 4096\n",
    "state_limits = []\n",
    "for i in range(env.gym_env.observation_space.shape[0]):\n",
    "    state_limits.append([\n",
    "        env.gym_env.observation_space.low[i],\n",
    "        env.gym_env.observation_space.high[i],\n",
    "    ])\n",
    "\n",
    "state_representation = StateRepresentation(method='tilecoding',\n",
    "                                           settings={\n",
    "                                               'num_tiles': num_tiles,\n",
    "                                               'num_tilings': num_tilings,\n",
    "                                               'iht_size': iht_size,\n",
    "                                               'state_limits': state_limits,\n",
    "                                           })\n",
    "\n",
    "states = np.zeros((iht_size, 1))\n",
    "\n",
    "actions = list(env.action_dict.keys())\n",
    "\n",
    "policy = {\n",
    "    'initialize': {\n",
    "        'file_path': './src/pytorch_networks/linear_discrete_actions.py',\n",
    "    },\n",
    "}\n",
    "\n",
    "value_network = {\n",
    "    'initialize': {\n",
    "        'file_path': './src/pytorch_networks/linear_discrete_actions.py',\n",
    "    }, \n",
    "}\n",
    "\n",
    "agent = RLAgent(agent_type='ac', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                avg_reward_method='differential',\n",
    "                initial_avg_reward=0.0,\n",
    "                action_type='discrete',\n",
    "                action_selection_rule='softmax',\n",
    "                policy_type='nn_pytorch',\n",
    "                policy_update_type='stochastic_gradient_descent',\n",
    "                policy_loss='ac_policy_loss',\n",
    "                value_type='nn_pytorch', \n",
    "                value_network=value_network,\n",
    "                value_update_type='stochastic_gradient_descent',\n",
    "                value_loss='mse_loss',\n",
    "                pytorch_device=pytorch_device,\n",
    "               )\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "\n",
    "step_sizes = {\n",
    "    'value': 0.002,\n",
    "    'policy': 2,\n",
    "    'avg_reward': 0.01,\n",
    "}\n",
    "\n",
    "df_pendulum_diff = rl_experiments.run_experiment_continuing(experiment='pendulum_diff',\n",
    "                                                            agent=agent, \n",
    "                                                            env=env,\n",
    "                                                            state_representation=state_representation,\n",
    "                                                            num_runs=10,\n",
    "                                                            max_steps=25000,\n",
    "                                                            discount=1.0,\n",
    "                                                            step_size=step_sizes,\n",
    "                                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97931e69",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17fdc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\n",
    "    'Differential': {\n",
    "        'df': df_pendulum_diff,\n",
    "        'color_cvar': '#007FA3',\n",
    "        'color_average': '#2FD1FF',\n",
    "    },\n",
    "    'RED CVaR': {\n",
    "        'df': df_pendulum_cvar,\n",
    "        'color_cvar': '#AB1368',\n",
    "        'color_average': '#EC52A8',\n",
    "    },\n",
    "}\n",
    "\n",
    "rl_experiments = RLExperiments()\n",
    "rl_experiments.get_performance_figure(experiment='pendulum',\n",
    "                                      df_dict=df_dict, \n",
    "                                      rolling_average_amount=1000,\n",
    "                                      x_max=6900,\n",
    "                                      quantile=0.1,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = EnvironmentPendulum(render_mode=None) \n",
    "\n",
    "# define agent with function approximation\n",
    "num_tiles = 8\n",
    "num_tilings = 32\n",
    "iht_size = 4096\n",
    "state_limits = []\n",
    "for i in range(env.gym_env.observation_space.shape[0]):\n",
    "    state_limits.append([\n",
    "        env.gym_env.observation_space.low[i],\n",
    "        env.gym_env.observation_space.high[i],\n",
    "    ])\n",
    "\n",
    "state_representation = StateRepresentation(method='tilecoding',\n",
    "                                           settings={\n",
    "                                               'num_tiles': num_tiles,\n",
    "                                               'num_tilings': num_tilings,\n",
    "                                               'iht_size': iht_size,\n",
    "                                               'state_limits': state_limits,\n",
    "                                           })\n",
    "\n",
    "states = np.zeros((iht_size, 1))\n",
    "\n",
    "actions = list(env.action_dict.keys())\n",
    "\n",
    "policy = {\n",
    "    'initialize': {\n",
    "        'file_path': './src/pytorch_networks/linear_discrete_actions.py',\n",
    "    },\n",
    "}\n",
    "\n",
    "value_network = {\n",
    "    'initialize': {\n",
    "        'file_path': './src/pytorch_networks/linear_discrete_actions.py',\n",
    "    }, \n",
    "}\n",
    "\n",
    "agent = RLAgent(agent_type='ac', \n",
    "                states=states, \n",
    "                actions=actions, \n",
    "                policy=policy, \n",
    "                avg_reward_method='differential',\n",
    "                initial_avg_reward=0.0,\n",
    "                action_type='discrete',\n",
    "                action_selection_rule='softmax',\n",
    "                policy_type='nn_pytorch',\n",
    "                policy_update_type='stochastic_gradient_descent',\n",
    "                policy_loss='ac_policy_loss',\n",
    "                value_type='nn_pytorch', \n",
    "                value_network=value_network,\n",
    "                value_update_type='stochastic_gradient_descent',\n",
    "                value_loss='mse_loss',\n",
    "                pytorch_device=pytorch_device,\n",
    "                use_cvar=True, \n",
    "                var_quantile=0.1, \n",
    "                initial_var_reward=0.0,\n",
    "               )\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "\n",
    "step_sizes = {\n",
    "    'value': 0.002,\n",
    "    'policy': 1,\n",
    "    'avg_reward': 0.01,\n",
    "    'var': 0.001,\n",
    "}\n",
    "\n",
    "rl_experiments.cvar_pendulum_estimates(agent=agent,\n",
    "                                       env=env,\n",
    "                                       experiment='cvar_pendulum_estimates',\n",
    "                                       state_representation=state_representation,\n",
    "                                       num_runs=1,\n",
    "                                       max_steps=500000,\n",
    "                                       discount=1.0,\n",
    "                                       step_size=step_sizes,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9136a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
