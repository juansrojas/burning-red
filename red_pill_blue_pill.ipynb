{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1753b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from src.rl_agent import RLAgent\n",
    "from src.rl_experiments import RLExperiments\n",
    "from src.state_representation import StateRepresentation\n",
    "\n",
    "from src.environments.env_pendulum import EnvironmentPendulum\n",
    "from src.environments.env_redpillbluepill import EnvironmentRedPillBluePill\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# pytorch_device = 'cuda'\n",
    "# pytorch_device = 'mps'\n",
    "pytorch_device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e773e23",
   "metadata": {},
   "source": [
    "## Red-Pill Blue-Pill (CVaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d8232",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For single tau\n",
    "\n",
    "# define environment\n",
    "env = EnvironmentRedPillBluePill(render_mode=None) \n",
    "\n",
    "# define agent\n",
    "actions = list(env.action_dict.keys())\n",
    "states = list(env.state_dict.values())\n",
    " \n",
    "policy = None\n",
    "\n",
    "agent = RLAgent(agent_type='q_learning',\n",
    "                states=states,\n",
    "                actions=actions,\n",
    "                policy=policy,\n",
    "                avg_reward_method='differential',\n",
    "                initial_avg_reward=0.0,\n",
    "                action_type='discrete',\n",
    "                action_selection_rule='epsilon_greedy',\n",
    "                policy_type='tabular',\n",
    "                value_type='tabular',\n",
    "                pytorch_device=pytorch_device,\n",
    "                use_cvar=True, \n",
    "                var_quantile=0.25, \n",
    "                initial_var_reward=0.0,\n",
    "               )\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "\n",
    "step_sizes = {\n",
    "    'value': 0.02,\n",
    "    'avg_reward': 0.1,\n",
    "    'var': 0.1,\n",
    "}\n",
    "\n",
    "df_rpbp_cvar = rl_experiments.run_experiment_continuing(experiment='redpillbluepill_cvar',\n",
    "                                                        agent=agent, \n",
    "                                                        env=env,\n",
    "                                                        num_runs=50,\n",
    "                                                        max_steps=100000,\n",
    "                                                        discount=1.0,\n",
    "                                                        epsilon=0.1,\n",
    "                                                        step_size=step_sizes,\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215bbbfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For multiple taus\n",
    "\n",
    "# define environment\n",
    "env = EnvironmentRedPillBluePill(render_mode=None) \n",
    "\n",
    "# define agent\n",
    "actions = list(env.action_dict.keys())\n",
    "states = list(env.state_dict.values())\n",
    " \n",
    "policy = None\n",
    "\n",
    "tau_experiment_results = {}\n",
    "for tau in [0.1, 0.25, 0.5, 0.75, 0.85, 0.9]:\n",
    "    print(tau)\n",
    "    agent = RLAgent(agent_type='q_learning',\n",
    "                states=states,\n",
    "                actions=actions,\n",
    "                policy=policy,\n",
    "                avg_reward_method='differential',\n",
    "                initial_avg_reward=0.0,\n",
    "                action_type='discrete',\n",
    "                action_selection_rule='epsilon_greedy',\n",
    "                policy_type='tabular',\n",
    "                value_type='tabular',\n",
    "                pytorch_device=pytorch_device,\n",
    "                use_cvar=True, \n",
    "                var_quantile=tau, \n",
    "                initial_var_reward=0.0,\n",
    "               )\n",
    "\n",
    "    # run experiment\n",
    "    rl_experiments = RLExperiments()\n",
    "\n",
    "    step_sizes = {\n",
    "        'value': 0.02,\n",
    "        'avg_reward': 0.1,\n",
    "        'var': 0.1,\n",
    "    }\n",
    "\n",
    "    df_rpbp_tau = rl_experiments.run_experiment_continuing(experiment='redpillbluepill_cvar_tau',\n",
    "                                                           agent=agent, \n",
    "                                                           env=env,\n",
    "                                                           num_runs=50,\n",
    "                                                           max_steps=500000,\n",
    "                                                           discount=1.0,\n",
    "                                                           epsilon=0.1,\n",
    "                                                           step_size=step_sizes,\n",
    "                                                          )\n",
    "\n",
    "    tau_experiment_results[tau] = df_rpbp_tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d42f98",
   "metadata": {},
   "source": [
    "## Red-Pill Blue-Pill (Differential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae2afc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = EnvironmentRedPillBluePill(render_mode=None) \n",
    "\n",
    "# define agent\n",
    "actions = list(env.action_dict.keys())\n",
    "states = list(env.state_dict.values())\n",
    " \n",
    "policy = None\n",
    "\n",
    "agent = RLAgent(agent_type='q_learning',\n",
    "                states=states,\n",
    "                actions=actions,\n",
    "                policy=policy,\n",
    "                avg_reward_method='differential',\n",
    "                initial_avg_reward=0.0,\n",
    "                action_type='discrete',\n",
    "                action_selection_rule='epsilon_greedy',\n",
    "                policy_type='tabular',\n",
    "                value_type='tabular',\n",
    "                pytorch_device=pytorch_device,\n",
    "               )\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "\n",
    "step_sizes = {\n",
    "    'value': 0.0002,\n",
    "    'avg_reward': 1,\n",
    "}\n",
    "\n",
    "df_rpbp_diff = rl_experiments.run_experiment_continuing(experiment='redpillbluepill_diff',\n",
    "                                                        agent=agent, \n",
    "                                                        env=env,\n",
    "                                                        num_runs=50,\n",
    "                                                        max_steps=100000,\n",
    "                                                        discount=1.0,\n",
    "                                                        epsilon=0.1,\n",
    "                                                        step_size=step_sizes,\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97931e69",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\n",
    "    'Differential': {\n",
    "        'df': df_rpbp_diff,\n",
    "        'color_cvar': '#007FA3',\n",
    "        'color_average': '#2FD1FF',\n",
    "    },\n",
    "    'RED CVaR': {\n",
    "        'df': df_rpbp_cvar,\n",
    "        'color_cvar': '#AB1368',\n",
    "        'color_average': '#EC52A8',\n",
    "    },\n",
    "}\n",
    "\n",
    "rl_experiments = RLExperiments()\n",
    "rl_experiments.get_performance_figure(experiment='redpillbluepill',\n",
    "                                      df_dict=df_dict, \n",
    "                                      rolling_average_amount=1000,\n",
    "                                      x_max=49900,\n",
    "                                      quantile=0.25,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = EnvironmentRedPillBluePill(render_mode=None) \n",
    "\n",
    "# define agent\n",
    "actions = list(env.action_dict.keys())\n",
    "states = list(env.state_dict.values())\n",
    " \n",
    "policy = None\n",
    "\n",
    "agent = RLAgent(agent_type='q_learning',\n",
    "                states=states,\n",
    "                actions=actions,\n",
    "                policy=policy,\n",
    "                avg_reward_method='differential',\n",
    "                initial_avg_reward=0.0,\n",
    "                action_type='discrete',\n",
    "                action_selection_rule='epsilon_greedy',\n",
    "                policy_type='tabular',\n",
    "                value_type='tabular',\n",
    "                pytorch_device=pytorch_device,\n",
    "                use_cvar=True, \n",
    "                var_quantile=0.25, \n",
    "                initial_var_reward=0.0,\n",
    "               )\n",
    "\n",
    "# run experiment\n",
    "rl_experiments = RLExperiments()\n",
    "\n",
    "step_sizes = {\n",
    "    'value': 0.02,\n",
    "    'avg_reward': 0.1,\n",
    "    'var': 0.1,\n",
    "}\n",
    "\n",
    "rl_experiments.cvar_redpillbluepill_estimates(agent=agent,\n",
    "                                              env=env,\n",
    "                                              experiment='cvar_rpbp_estimates',\n",
    "                                              num_runs=1,\n",
    "                                              max_steps=100000,\n",
    "                                              discount=1.0,\n",
    "                                              epsilon=0.1,\n",
    "                                              step_size=step_sizes,\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e85879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CVaR values of red and blue policies (estimated using monte carlo)\n",
    "rl_experiments = RLExperiments()\n",
    "rl_experiments.get_cvar_by_tau_plot(\n",
    "    n_samples=100000,\n",
    "    epsillon=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c71c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau experiment\n",
    "rl_experiments = RLExperiments()\n",
    "rl_experiments.get_tau_results_figure(experiment='rpbp_by_tau',\n",
    "                                      results_dict=tau_experiment_results, \n",
    "                                      n_runs=50, \n",
    "                                      rolling_average_amount=2500,\n",
    "                                      x_max=500000,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9136a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
